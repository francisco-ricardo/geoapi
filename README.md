# GeoSpatial Links API

[![FastAPI](https://img.shields.io/badge/FastAPI-0.115.1-009688.svg?style=flat-square&logo=fastapi)](https://fastapi.tiangolo.com/)
[![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-2.0.41-red.svg?style=flat-square&logo=sqlalchemy)](https://www.sqlalchemy.org/)
[![Python](https://img.shields.io/badge/Python-3.12-3776ab.svg?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-336791.svg?style=flat-square&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![PostGIS](https://img.shields.io/badge/PostGIS-3.5-4CAF50.svg?style=flat-square)](https://postgis.net/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg?style=flat-square&logo=docker&logoColor=white)](https://www.docker.com/)
[![Pydantic](https://img.shields.io/badge/Pydantic-2.11.1-E92063.svg?style=flat-square&logo=pydantic&logoColor=white)](https://docs.pydantic.dev/latest/)

[![pytest](https://img.shields.io/badge/pytest-8.4.1-0A9EDC.svg?style=flat-square&logo=pytest&logoColor=white)](https://docs.pytest.org/)
[![Type Checking](https://img.shields.io/badge/Type%20Checking-mypy-1674b1.svg?style=flat-square&logo=mypy&logoColor=white)](http://mypy-lang.org/)

[![Pandas](https://img.shields.io/badge/Pandas-2.3.0-150458.svg?style=flat-square&logo=pandas&logoColor=white)](https://pandas.pydata.org/)
[![Shapely](https://img.shields.io/badge/Shapely-2.1.1-blue.svg?style=flat-square)](https://shapely.readthedocs.io/)
[![GeoAlchemy2](https://img.shields.io/badge/GeoAlchemy2-0.17.1-green.svg?style=flat-square)](https://geoalchemy-2.readthedocs.io/)
[![Geospatial](https://img.shields.io/badge/Geospatial-GeoJSON-brightgreen.svg?style=flat-square)](https://geojson.org/)
[![PyArrow](https://img.shields.io/badge/PyArrow-20.0.0-red.svg?style=flat-square)](https://arrow.apache.org/docs/python/)

[![API Docs](https://img.shields.io/badge/API%20Docs-Swagger-85EA2D.svg?style=flat-square&logo=swagger&logoColor=white)](http://localhost:8000/docs)
[![DevContainer](https://img.shields.io/badge/DevContainer-Ready-purple.svg?style=flat-square&logo=visualstudiocode&logoColor=white)](.devcontainer/)
[![Architecture](https://img.shields.io/badge/Architecture-Clean-blue.svg?style=flat-square)](#project-structure)

[![Throughput](https://img.shields.io/badge/Throughput-3K%20Records%2Fs-orange.svg?style=flat-square)](#performance-optimization-big-data-ingestion)
[![Big Data](https://img.shields.io/badge/Big%20Data-1.2M%2B%20Records-red.svg?style=flat-square)](#performance-optimization-big-data-ingestion)
[![Reliability](https://img.shields.io/badge/Reliability-Zero%20Failures-success.svg?style=flat-square)](#performance-optimization-big-data-ingestion)

A robust geospatial REST API built with **FastAPI**, **SQLAlchemy**, **PostgreSQL/PostGIS**, and **Pydantic** for traffic data analysis and visualization.

## ğŸ† Project Highlights

### âœ¨ **Quality Achievements**
- **109 Tests**: Comprehensive test suite with 100% pass rate
- **Zero Technical Debt**: Clean, well-organized codebase following SOLID principles
- **Performance Optimized**: Handles 1.3M+ records efficiently with chunked processing
- **Production Ready**: Docker containerized with health checks and monitoring

### ğŸš€ **Technical Excellence**
- **Clean Architecture**: Domain-driven design with clear separation of concerns
- **Type Safety**: Full typing support with mypy validation
- **Observability**: Structured logging with correlation IDs and request tracing
- **Geospatial Ready**: PostGIS integration with GeoJSON support
- **DevOps Ready**: Complete Docker setup with development containers

### ğŸ“Š **Data Processing Capabilities**
- **Big Data Handling**: Optimized for processing millions of records
- **Memory Efficient**: Chunked processing with automatic garbage collection
- **Integrity Validation**: Comprehensive data validation and consistency checks
- **Multiple Formats**: Support for Parquet, GeoJSON, and standard database formats
- **Interactive Visualization**: Jupyter notebook with MapboxGL integration for data exploration

---

## ğŸ”Œ Technologies

### Core Stack
- **Backend**: FastAPI, SQLAlchemy 2.0, Pydantic v2
- **Database**: PostgreSQL + PostGIS (with automatic table creation)
- **Geospatial**: GeoAlchemy2, PostGIS, GeoJSON
- **Testing**: pytest, TDD approach with 109 tests
- **DevOps**: Docker, DevContainer, automated setup

### Development Tools
- **Code Quality**: Black (formatting), mypy (type checking)
- **Testing**: pytest with fixtures, parametrized tests, coverage reports
- **Documentation**: FastAPI auto-docs, Swagger UI, comprehensive docstrings
- **Performance**: SQLAlchemy bulk operations, memory-optimized data processing
- **Observability**: Structured logging, correlation IDs, request tracing

---

## ğŸ—ï¸ Architecture

The project follows **Clean Architecture**, **SOLID**, and **KISS** principles with a layered approach:

### ğŸ“Š **Application Layers**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                API Layer                â”‚  â† FastAPI endpoints, validation
â”‚         (app/api/v1/*.py)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Middleware Layer             â”‚  â† Request/response processing
â”‚        (app/middleware/*.py)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Schema Layer               â”‚  â† Pydantic models, serialization
â”‚          (app/schemas/*.py)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Services Layer               â”‚  â† Business logic
â”‚          (app/services/*.py)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Model Layer                â”‚  â† SQLAlchemy ORM, relationships
â”‚          (app/models/*.py)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Core Layer                 â”‚  â† Database, config, logging
â”‚           (app/core/*.py)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ **Design Patterns Implemented**

- **Factory Pattern**: Database engine and session creation (`get_engine()`, `get_session_factory()`)
- **Dependency Injection**: FastAPI DI system for configuration and database dependencies
- **Middleware Pattern**: Request logging and correlation IDs (`LoggingMiddleware`)
- **Strategy Pattern**: Environment-specific configurations and database adapters
- **Singleton Pattern**: Cached database engine and logger instances

### ğŸ—ï¸ **System Architecture & Data Flow**

The following diagram shows the complete system architecture, including data sources, ingestion pipeline, database integration, API services, and visualization components:

```mermaid
graph TB
    %% External Data Sources
    subgraph "Data Sources"
        DS1["ğŸŒ Link Info Dataset"]
        DS2["ğŸŒ Speed Data Dataset"]
    end

    %% Data Ingestion Layer
    subgraph "Data Ingestion Pipeline"
        DI["ğŸ“ ingest_datasets.py"]
        DV["âœ… validate_ingestion.py"]
    end

    %% Database Layer
    subgraph "Database Layer"
        subgraph "PostgreSQL PostGIS"
            DB[("ğŸ—ƒï¸ PostgreSQL 16 + PostGIS 3.5")]
            LT["ğŸ“‹ links Table"]
            ST["ğŸ“Š speed_records Table"]
        end
    end

    %% FastAPI Application Layer
    subgraph "FastAPI Application"
        subgraph "Core Layer"
            CORE["âš™ï¸ Core Services"]
        end
        
        subgraph "Model Layer"
            MODEL["ğŸ—ï¸ SQLAlchemy Models"]
        end
        
        subgraph "Services Layer"
            SERV["ğŸ§® Business Logic"]
        end
        
        subgraph "Schema Layer"
            SCHEMA["ğŸ“‹ Pydantic Schemas"]
        end
        
        subgraph "Middleware Layer"
            MIDDLE["ğŸ›¡ï¸ Request Processing"]
        end
        
        subgraph "API Layer"
            API["ğŸŒ REST Endpoints"]
        end
    end

    %% Consumption Layer
    subgraph "Data Consumption"
        NB["ğŸ““ Jupyter Notebook"]
        CLI["ğŸ’» API Clients"]
        DOCS["ğŸ“š Interactive Docs"]
    end

    %% Data Flow Arrows
    DS1 -->|Download & Parse| DI
    DS2 -->|Download & Parse| DI
    DI -->|Bulk Insert| LT
    DI -->|Bulk Insert 1.3M+ Records| ST
    DI -->|Validation| DV
    DV -->|Integrity Check| DB
    
    LT -.->|1:N Relationship| ST
    
    CORE -->|Connection Pool| DB
    MODEL -->|ORM Queries| DB
    SERV -->|Business Logic| MODEL
    SCHEMA -->|Validation| SERV
    MIDDLE -->|Processing| SCHEMA
    API -->|HTTP Layer| MIDDLE
    
    API -->|JSON Responses| NB
    API -->|REST API| CLI
    API -->|Documentation| DOCS
    
    %% Styling
    classDef dataSource fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef processing fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef database fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef fastapi fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef consumption fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class DS1,DS2 dataSource
    class DI,DV processing
    class DB,LT,ST database
    class CORE,MODEL,SERV,SCHEMA,MIDDLE,API fastapi
    class NB,CLI,DOCS consumption
```

**ğŸ“‹ Architecture Highlights:**

- **ğŸ”„ Data Pipeline**: Complete ETL process from external Parquet files to PostGIS database
- **ğŸ—ï¸ Clean Architecture**: Layered FastAPI application following SOLID principles  
- **ğŸ—„ï¸ Spatial Database**: PostgreSQL + PostGIS with optimized indexes for geospatial queries
- **ğŸ“Š Big Data Processing**: Handles 1.3M+ records with chunked processing and memory optimization
- **ğŸŒ RESTful API**: FastAPI with automatic OpenAPI documentation and validation
- **ğŸ“ˆ Data Visualization**: Jupyter notebook with MapboxGL for interactive geospatial analysis
- **ğŸ” Observability**: Comprehensive logging, correlation IDs, and request tracing
- **ğŸ³ Containerized**: Full Docker setup with PostgreSQL and FastAPI services

---

## ğŸ“Š Code Quality

### Quality Metrics & Standards
- âœ… **Type Safety**: 100% mypy type checking coverage
- âœ… **Code Style**: Black formatting with consistent style
- âœ… **Import Sorting**: isort for clean import organization
- âœ… **Test Coverage**: 66% overall coverage with domain-specific targets
- âœ… **Architecture**: Clean architecture with domain separation
- âœ… **Documentation**: Comprehensive docstrings and API documentation

### Quality Commands
```bash
# Code formatting
make format              # Format code with Black
make format-check        # Check formatting without changes

# Type checking
make type-check          # Run mypy type checking
make type-check-strict   # Strict type checking

# Import sorting
make sort-imports        # Sort imports with isort
make sort-imports-check  # Check import sorting

# Combined quality check
make quality-check       # Run all quality checks
```

### Code Quality Tools
| Tool | Purpose | Configuration | Status |
|------|---------|---------------|--------|
| **Black** | Code formatting | Line length: 88 | âœ… Configured |
| **mypy** | Type checking | Strict mode | âœ… Configured |
| **isort** | Import sorting | Black compatible | âœ… Configured |
| **pytest** | Testing framework | Coverage enabled | âœ… 109 tests |

### Quality Gates
- All code must pass Black formatting
- All code must pass mypy type checking
- All tests must pass (100% success rate)
- New code should maintain or improve coverage
- All commits should follow conventional commit format

---

## ğŸ—„ï¸ Database Schema

The GeoAPI uses a well-designed relational schema optimized for geospatial traffic data. The database consists of two main entities with a one-to-many relationship.

### Entity Relationship Diagram

![Database Schema - Entity Relationship Diagram](docs/geoapi_der.drawio.png)

### Schema Overview

**Links Table (`links`)**
- Stores road segment information with PostGIS geometry
- Primary key: `link_id` (integer)
- Contains road metadata: name, type, speed limit, length
- Geometry stored as `LINESTRING` in WGS84 (SRID 4326)

**Speed Records Table (`speed_records`)**
- Stores traffic speed measurements
- Foreign key reference to `links.link_id`
- Contains temporal data: timestamp, day of week, time period
- Speed values in miles per hour (mph)

### Key Features

- **Referential Integrity**: All speed records reference valid links with CASCADE delete
- **Spatial Indexing**: Optimized GIST indexes on geometry columns for fast spatial queries
- **Temporal Indexing**: Indexes on timestamp and temporal classification fields
- **Data Validation**: Built-in constraints ensure data quality (speed ranges, positive lengths, etc.)
- **PostGIS Integration**: Full spatial data support with geometry validation and transformation

### Database Technologies

- **PostgreSQL 16**: Primary database engine
- **PostGIS 3.5**: Geospatial extension for spatial data types and operations
- **SQLAlchemy 2.0**: ORM with modern async support
- **GeoAlchemy2**: Spatial extension for SQLAlchemy with PostGIS integration

---

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose installed on your host machine
- Clone this repository

### ğŸ”§ Environment Configuration

**âš ï¸ CRITICAL**: You must create a `.env` file in the project root before starting the containers. The application will not work without proper environment configuration.

#### Quick Setup Command

```bash
# Create .env file with default configuration
# Replace `your_mapbox_token_here` with your token
cat > .env << 'EOF'
GEOAPI_DATABASE_URL=postgresql://geoapi:geoapi@db:5432/geoapi
GEOAPI_API_HOST=0.0.0.0
GEOAPI_API_PORT=8000
GEOAPI_DEBUG=true
GEOAPI_APP_NAME=GeoAPI
GEOAPI_APP_VERSION=1.0.0
GEOAPI_MAPBOX_ACCESS_TOKEN=your_mapbox_token_here
GEOAPI_LINK_INFO_URL=https://cdn.urbansdk.com/data-engineering-interview/link_info.parquet.gz
GEOAPI_SPEED_DATA_URL=https://cdn.urbansdk.com/data-engineering-interview/duval_jan1_2024.parquet.gz
EOF
```

### Setup and Run
```bash
# 1. Complete setup (start the containers + create tables + data ingestion)
make setup
make validate-ingestion # Validate data ingestion integrity

# 2. Start the API with uvicorn
make run-api-dev        # Recommended for development (auto-reload + debug)

# 3. Access the API
# - API Server: http://localhost:8000
# - API Documentation: http://localhost:8000/docs
# - Health Check: http://localhost:8000/health
```

> **ğŸ“‹ Note**: The API container starts but doesn't auto-run the FastAPI app, giving you control over when and how to start it (dev/prod mode). This is an
approach for better debugging and flexibility.

### Quick Verification
```bash
# Check if everything is working
make api-status         # Complete status check
make check-api          # Quick API health check
make test               # Run unit tests
```

### ğŸ“ Command Summary

| Command | Description | Use Case |
|---------|-------------|----------|
| `make start` | Start containers (DB + API container) | Initial setup |
| `make setup` | Complete setup (start + tables + data) | First time setup |
| `make run-api-dev` | Start FastAPI with auto-reload + debug | **Development (RECOMMENDED)** |
| `make run-api` | Start FastAPI with auto-reload | Basic usage |
| `make check-api` | Quick API health check | Verify API works |
| `make api-status` | Complete status (container + API + endpoints) | Troubleshooting |
| `make test` | Run unit tests | Verify functionality |
| `make logs` | View container logs | Debugging |

### Alternative Commands
```bash
# Step by step setup
make start              # Start containers (DB + prepare API container)
make create-tables      # Create database tables
make ingest-data        # Load Parquet datasets

# API Management
make run-api-dev        # Start FastAPI in development mode (RECOMMENDED)
make check-api          # Check if API is responding
make stop-api           # Stop API process (uvicorn)
make restart-api        # Restart API process
make api-status         # Show API status and endpoints

# Development commands
make logs              # View container logs
make shell             # Open shell in API container
make db-shell          # Open PostgreSQL shell
make test              # Run tests
make health-check      # Check system health

# Data validation
make validate-ingestion # Validate data ingestion integrity
```

### API Development Workflow

```bash
# Quick Start for API Development
make start              # 1. Start containers (PostgreSQL + API container)
make run-api-dev        # 2. Start FastAPI with auto-reload and debug
make check-api          # 3. Verify API is responding

# Daily Development Cycle
make run-api-dev        # Start API in development mode
# Make code changes... (auto-reload active)
make test               # Run tests
make check-api          # Verify API still works

# API Status and Debugging
make api-status         # Show complete API status
make logs               # View container logs for debugging
make restart-api        # Restart if needed
```

### API Management Commands

| Command | Purpose | When to Use |
|---------|---------|-------------|
| `make run-api-dev` | Start FastAPI with auto-reload + debug | **Development** (recommended) |
| `make run-api` | Start FastAPI with auto-reload | Basic development |
| `make run-api-prod` | Start FastAPI with 4 workers | Production testing |
| `make check-api` | Test if API responds | Quick health check |
| `make api-status` | Complete status (container + API + endpoints) | Troubleshooting |
| `make stop-api` | Stop uvicorn process | Stop API without stopping containers |
| `make restart-api` | Restart API process | After configuration changes |

### Access Points
- **API Server**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs  
- **Health Check**: http://localhost:8000/health

---

## ï¿½ API Endpoints

The GeoAPI provides 4 main endpoints for traffic data analysis and geospatial querying. All endpoints return JSON responses and follow RESTful conventions.

### 1. ğŸ“Š GET `/aggregates/` - Daily Speed Aggregates

Get aggregated speed data for all links on a specific day and time period.

**Parameters:**
- `day` (required): Day of the week ("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
- `period` (required): Time period ("Overnight", "Early Morning", "AM Peak", "Midday", "Early Afternoon", "PM Peak", "Evening")

**Example Request:**
```bash
# Get AM Peak data for Monday
curl "http://localhost:8000/aggregates/?day=Monday&period=AM%20Peak"

# Get Evening data for Friday
curl "http://localhost:8000/aggregates/?day=Friday&period=Evening"
```

**Example Response:**
```json
[
  {
    "link_id": 16981048,
    "road_name": "Philips Hwy",
    "length": 0.009320565,
    "road_type": null,
    "speed_limit": null,
    "geometry": {
      "type": "LineString",
      "coordinates": [[-81.59791, 30.24124], [-81.59801, 30.24135]]
    },
    "average_speed": 45.4,
    "record_count": 3,
    "min_speed": 43.0,
    "max_speed": 47.35,
    "speed_stddev": 2.21
  }
]
```

### 2. ğŸ” GET `/aggregates/{link_id}` - Single Link Data

Get detailed speed data for a specific link.

**Parameters:**
- `link_id` (path): The numeric ID of the link
- `day` (required): Day of the week ("Monday", "Tuesday", etc.)
- `period` (required): Time period ("AM Peak", "PM Peak", "Midday", etc.)

**Example Request:**
```bash
# Get AM Peak data for link 16981048 on Monday
curl "http://localhost:8000/aggregates/16981048?day=Monday&period=AM%20Peak"

# Get Evening data for link 16981074 on Wednesday
curl "http://localhost:8000/aggregates/16981074?day=Wednesday&period=Evening"
```

**Example Response:**
```json
{
  "link_id": 16981048,
  "road_name": "Philips Hwy",
  "length": 0.009320565,
  "road_type": null,
  "speed_limit": null,
  "geometry": {
    "type": "LineString",
    "coordinates": [[-81.59791, 30.24124], [-81.59801, 30.24135]]
  },
  "average_speed": 45.4,
  "record_count": 3,
  "min_speed": 43.0,
  "max_speed": 47.35,
  "speed_stddev": 2.21
}
```

### 3. ğŸŒ GET `/patterns/slow_links/` - Slow Traffic Patterns

Find links with consistently slow traffic patterns.

**Parameters:**
- `period` (required): Time period ("AM Peak", "PM Peak", "Midday", etc.)
- `threshold` (required): Maximum average speed to consider "slow" (mph)
- `min_days` (required): Minimum number of days the link must be slow (1-7)

**Example Request:**
```bash
# Find links slower than 15 mph during AM Peak for at least 3 days per week
curl "http://localhost:8000/patterns/slow_links/?period=AM%20Peak&threshold=15&min_days=3"

# Find links slower than 25 mph during PM Peak for at least 2 days per week
curl "http://localhost:8000/patterns/slow_links/?period=PM%20Peak&threshold=25&min_days=2"
```

**Example Response:**
```json
[
  {
    "link_id": 1313272474,
    "road_name": "Oyster Creek Rd",
    "length": 0.176469364,
    "road_type": null,
    "speed_limit": null,
    "geometry": {
      "type": "LineString",
      "coordinates": [[-81.59376, 30.44053], [-81.5937, 30.44062]]
    },
    "average_speed": 10.31,
    "record_count": 1,
    "min_speed": 10.31,
    "max_speed": 10.31,
    "speed_stddev": null
  }
]
```

### 4. ğŸ—ºï¸ POST `/aggregates/spatial_filter/` - Spatial Query

Get aggregated data for links within a bounding box area.

**Request Body:**
```json
{
  "day": "Wednesday",
  "period": "AM Peak",
  "bbox": [-81.8, 30.1, -81.6, 30.3]
}
```

**Example Request:**
```bash
curl -X POST "http://localhost:8000/aggregates/spatial_filter/" \
  -H "Content-Type: application/json" \
  -d '{
    "day": "Wednesday",
    "period": "AM Peak",
    "bbox": [-81.8, 30.1, -81.6, 30.3]
  }'
```

**Example Response:**
```json
[
  {
    "link_id": 1313709937,
    "road_name": "Walkers Ridge Dr",
    "length": 0.259733078,
    "road_type": null,
    "speed_limit": null,
    "geometry": {
      "type": "LineString",
      "coordinates": [[-81.81882, 30.24779], [-81.81897, 30.25156]]
    },
    "average_speed": 13.98,
    "record_count": 1,
    "min_speed": 13.98,
    "max_speed": 13.98,
    "speed_stddev": null
  }
]
```

### ğŸš€ Quick API Testing

```bash
# 1. Test API health
curl "http://localhost:8000/health"

# 2. Get Monday AM Peak traffic data  
curl "http://localhost:8000/aggregates/?day=Monday&period=AM%20Peak"

# 3. Get data for a specific link
curl "http://localhost:8000/aggregates/16981048?day=Monday&period=AM%20Peak"

# 4. Find consistently slow links (under 15 mph during AM Peak for 2+ days)
curl "http://localhost:8000/patterns/slow_links/?period=AM%20Peak&threshold=15&min_days=2"

# 5. Get data within a geographic area
curl -X POST "http://localhost:8000/aggregates/spatial_filter/" \
  -H "Content-Type: application/json" \
  -d '{"day": "Wednesday", "period": "AM Peak", "bbox": [-81.8, 30.1, -81.6, 30.3]}'
```

### ğŸ“– Interactive API Documentation

Visit http://localhost:8000/docs for interactive Swagger documentation where you can:
- Test all endpoints directly in your browser
- View detailed parameter descriptions
- See response schemas and examples
- Download OpenAPI specification

### ğŸ“Š Data Visualization Notebook

The project includes a **Jupyter Notebook** demonstrating real-world API usage with geospatial visualization:

**ğŸ“ Location**: `notebooks/notebook_1.ipynb`

**ğŸ¯ Purpose**: Complete example showing how to:
- Consume API endpoints programmatically
- Process geospatial traffic data
- Create interactive maps with Mapbox GL
- Generate data analysis summaries

**ğŸ”§ Features**:
- **API Integration**: Demonstrates GET requests to `/aggregates/` endpoint
- **MapboxGL Visualization**: Interactive choropleth maps showing traffic speeds
- **Data Analysis**: Pandas-based data processing and tabular summaries
- **Real Data**: Uses Monday AM Peak traffic data from Jacksonville, FL

**ğŸš€ Quick Start**:
```bash
# Start the API first
make setup && make run-api-dev

# Open the notebook in VS Code or Jupyter
code notebooks/notebook_1.ipynb

# Or run Jupyter Lab
jupyter lab notebooks/notebook_1.ipynb
```

**ğŸ“‹ Requirements**:
- API running on `http://localhost:8000` (or update `BASE_URL` in notebook)
- Optional: Mapbox token for advanced visualizations (placeholder provided)
- Python packages: `requests`, `pandas`, `mapboxgl`, `geopandas`, `shapely`

**ğŸ’¡ Use Cases**:
- **Interview Demonstrations**: Live API consumption and visualization
- **Development Testing**: Interactive testing of API endpoints
- **Data Analysis**: Exploratory analysis of traffic patterns
- **Client Examples**: Reference implementation for API consumers

> **Note**: This notebook follows the exact pattern specified in the original requirements, providing a complete example of API integration with geospatial visualization capabilities.

---

## ï¿½ğŸ”§ Development & Troubleshooting

### Common Development Tasks

```bash
# Start development environment
make start && make run-api-dev

# Run tests while developing
make test                    # Quick unit tests
make test-coverage          # Tests with coverage report
make test-api               # Test API endpoints

# Code quality checks
make format                 # Format code with Black
make type-check            # Type checking with mypy
make sort-imports          # Sort imports with isort
make quality-check         # All quality checks combined
```

### Troubleshooting Guide

#### âš™ï¸ Configuration Issues
```bash
# Check if .env file exists and has correct values
ls -la .env                # Verify .env file exists
cat .env                   # Check environment variables
make health-check          # Verify configuration is loaded correctly
```

**Common Configuration Problems:**
- âŒ **Missing `.env` file**: Create `.env` file in project root using the template above
- âŒ **Wrong database URL**: Ensure `GEOAPI_DATABASE_URL` matches Docker container settings
- âŒ **Invalid port configuration**: Check `GEOAPI_API_PORT` is set to `8000`
- âŒ **Missing data source URLs**: Verify both `GEOAPI_LINK_INFO_URL` and `GEOAPI_SPEED_DATA_URL` are set

#### API Not Responding
```bash
make api-status            # Check status
make logs                  # View logs
make restart-api           # Restart API process
```

#### Container Issues
```bash
make restart               # Restart all containers
make logs                  # Check container logs
make shell                 # Debug inside container
```

#### Database Issues
```bash
make db-shell              # Open PostgreSQL shell
make health-check          # Check database connectivity
make clean-db              # Clean database (careful!)
```

#### Development Issues
```bash
make clean-empty-files     # Remove VS Code empty files
make clean-pycache         # Clean Python cache
make format                # Fix code formatting
```

---

## ğŸ§ª Testing System
```bash
# Code formatting
make format              # Format code with Black
make format-check        # Check formatting without changes

# Type checking
make type-check          # Run mypy type checking
make type-check-strict   # Strict type checking

# Import sorting
make sort-imports        # Sort imports with isort
make sort-imports-check  # Check import sorting

# Combined quality check
make quality-check       # Run all quality checks
```

### Code Quality Tools
| Tool | Purpose | Configuration | Status |
|------|---------|---------------|--------|
| **Black** | Code formatting | Line length: 88 | âœ… Configured |
| **mypy** | Type checking | Strict mode | âœ… Configured |
| **isort** | Import sorting | Black compatible | âœ… Configured |
| **pytest** | Testing framework | Coverage enabled | âœ… 109 tests |

### Quality Gates
- All code must pass Black formatting
- All code must pass mypy type checking
- All tests must pass (100% success rate)
- New code should maintain or improve coverage
- All commits should follow conventional commit format

---

## ğŸ“‚ Project Structure

```
app/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py          # Configuration with Pydantic Settings
â”‚   â”œâ”€â”€ database.py        # Engine/session factory (SQLite/PostgreSQL)
â”‚   â”œâ”€â”€ exceptions.py      # Custom exception handlers
â”‚   â”œâ”€â”€ logging.py         # Structured logging system
â”‚   â””â”€â”€ time_periods.py    # Time period classifications
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ link.py           # Road links model (with PostGIS geometry)
â”‚   â””â”€â”€ speed_record.py   # Speed measurements model
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ aggregation.py    # Aggregation result schemas
â”‚   â”œâ”€â”€ link.py           # Pydantic schemas for links
â”‚   â””â”€â”€ speed_record.py   # Pydantic schemas for speed records
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ dependencies.py   # FastAPI dependencies
â”‚   â””â”€â”€ v1/
â”‚       â”œâ”€â”€ aggregates.py # Aggregation endpoints
â”‚       â””â”€â”€ links.py      # Link endpoints
â”œâ”€â”€ middleware/
â”‚   â””â”€â”€ logging_middleware.py # Request logging middleware
â”œâ”€â”€ services/
â”‚   â””â”€â”€ aggregation_service.py # Business logic layer
â””â”€â”€ main.py               # FastAPI application

scripts/
â”œâ”€â”€ setup/
â”‚   â””â”€â”€ complete_setup.py # Automated project setup
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ create_tables.py  # Database initialization
â”‚   â”œâ”€â”€ verify_database.py # Database verification
â”‚   â””â”€â”€ verify_postgis.py # PostGIS verification
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ analyze_data.py   # Data analysis utilities
â”‚   â”œâ”€â”€ ingest_datasets.py # Data ingestion pipeline
â”‚   â””â”€â”€ validate_ingestion.py # Data validation
â”œâ”€â”€ demo/
â”‚   â”œâ”€â”€ schemas_basic.py  # Basic schema demonstration
â”‚   â””â”€â”€ schemas_complete.py # Complete schema guide
â””â”€â”€ testing/
    â”œâ”€â”€ run_tests.py      # Test runner
    â”œâ”€â”€ run_tests_by_category.py # Category-based tests
    â””â”€â”€ test_endpoints.py # API endpoint testing

tests/
â”œâ”€â”€ conftest.py           # Global test fixtures
â”œâ”€â”€ fixtures/             # Shared test fixtures
â”‚   â””â”€â”€ models.py        # Model fixtures
â””â”€â”€ unit/                # Unit tests (organized by domain)
    â”œâ”€â”€ core/            # Core functionality tests
    â”œâ”€â”€ models/          # Model tests
    â”œâ”€â”€ schemas/         # Schema validation tests
    â””â”€â”€ middleware/      # Middleware tests
```

---

## ğŸ§ª Testing System

The project features a **comprehensive, well-organized testing system** with clean architecture following Domain-Driven Design principles:

### Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Global test fixtures
â”œâ”€â”€ fixtures/                # Shared test fixtures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ models.py           # Model fixtures
â””â”€â”€ unit/                   # Unit tests (organized by domain)
    â”œâ”€â”€ core/               # Core functionality tests
    â”‚   â”œâ”€â”€ test_database.py
    â”‚   â””â”€â”€ test_logging.py
    â”œâ”€â”€ models/             # Model tests
    â”‚   â”œâ”€â”€ test_link.py
    â”‚   â””â”€â”€ test_speed_record.py
    â”œâ”€â”€ schemas/            # Schema validation tests
    â”‚   â”œâ”€â”€ test_link.py
    â”‚   â””â”€â”€ test_speed_record.py
    â””â”€â”€ middleware/         # Middleware tests
        â””â”€â”€ test_logging_middleware.py
```

### Run Tests

```bash
# Run all unit tests
make test

# Run all unit tests (comprehensive)
make test-all

# Run by domain/category
make test-unit              # Unit tests only
make test-models           # Model tests only
make test-schemas          # Schema tests only
make test-core             # Core functionality tests
make test-middleware       # Middleware tests only
make test-api              # API endpoint tests

# Database and logging specific
make test-database         # Database tests only
make test-logging          # Logging system tests
```

### Test Coverage

```bash
# Basic coverage report
make test-coverage

# Detailed coverage with branch analysis
make test-coverage-detailed
```

### Coverage Reports

Coverage reports are generated in multiple formats:
- **Console output**: Real-time summary during test runs
- **HTML report**: Detailed interactive report at `coverage_html/index.html`
- **XML report**: CI-compatible report at `coverage.xml`

#### Coverage Targets & Current Status

| Component | Target | Current Status | Tests |
|-----------|--------|----------------|-------|
| Core Database | 95%+ | âœ… 91% | 12 tests |
| Core Logging | 95%+ | âœ… 94% | 36 tests |
| Models | 90%+ | âœ… 82% | 43 tests |
| Schemas | 95%+ | âœ… 100% | 33 tests |
| Middleware | 90%+ | âœ… 100% | 6 tests |
| **Overall** | **85%+** | âœ… **66%** | **109 tests** |

### Test Features

- **Clean Architecture**: Tests organized by domain (core, models, schemas, middleware)
- **Comprehensive Fixtures**: Reusable test fixtures in dedicated directory
- **Foreign Key Integrity**: Proper database relationship testing
- **Edge Case Coverage**: Extensive testing of boundary conditions
- **Type Safety**: Full typing support with proper SQLAlchemy integration
- **Fast Execution**: Optimized test suite with efficient database handling
- **100% Pass Rate**: All 109 tests pass consistently

---

## ğŸ” Data Validation

### Ingestion Integrity Validation

The project includes a robust data validation system to ensure integrity after ingestion:

```bash
# Validate data integrity after ingestion
make validate-ingestion
```

#### What is validated:

**1. Link Data:**
- Valid geometries (GeoJSON/WKT)
- Consistent coordinates (SRID 4326) 
- Required fields are populated
- Uniqueness of link_ids

**2. Speed Data:**
- Valid references to existing links
- Timestamps in correct format (UTC)
- Speed values within reasonable limits
- Time periods correctly categorized

**3. Referential Integrity:**
- All speed_records reference valid links
- Valid and consistent PostGIS geometries
- Uniform SRID across all geometries

**4. Statistical Consistency:**
- Expected record counts
- Consistent average speeds by period
- Proper temporal distribution

#### Sample Output:
```
[PASS] Link geometries validation passed
[PASS] Speed records validation passed  
[PASS] All speed records have valid link references
[PASS] All geometries use consistent SRID: 4326
[PASS] Average speed for AM Peak matches: 35.94 mph
[PASS] ALL VALIDATIONS PASSED - Data ingestion is accurate

*** Data integrity confirmed! The ingestion process worked correctly. ***
```

### Other Validation Commands

```bash
make analyze-data      # Analyze original Parquet datasets
make verify-db         # Verify database state
make verify-postgis    # Verify PostGIS spatial data
```

---

## ğŸ“‹ Logging and Observability

[![OpenTelemetry](https://img.shields.io/badge/OpenTelemetry-Ready-4287f5.svg?style=flat-square&logo=opentelemetry&logoColor=white)](https://opentelemetry.io/)
[![Correlation IDs](https://img.shields.io/badge/Correlation%20IDs-Enabled-green.svg?style=flat-square)](https://microservices.io/patterns/observability/distributed-tracing.html)
[![Structured Logging](https://img.shields.io/badge/Structured%20Logging-JSON%20%26%20Console-blue.svg?style=flat-square)](https://12factor.net/logs)
[![Cloud Ready](https://img.shields.io/badge/Cloud%20Ready-Observability-purple.svg?style=flat-square&logo=googlecloud&logoColor=white)](https://cloud.google.com/logging)

The API includes a comprehensive logging and observability system:

### Key Features

- **Structured Logging**: Supports both human-readable console logs and machine-parseable JSON format
- **Correlation IDs**: Every request gets a unique ID that is propagated through all logs
- **Request/Response Logging**: Automatic logging of all HTTP requests with timing and performance metrics
- **Cloud-Ready**: Designed for integration with cloud observability platforms
- **Contextual Logging**: Endpoint handlers can access request-scoped loggers with correlation IDs

### Configuration

Logging can be configured via environment variables:

```bash
# Logging configuration
GEOAPI_LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
GEOAPI_LOG_FORMAT=console  # console or json
GEOAPI_LOG_TO_FILE=false  # true or false
GEOAPI_LOG_FILE_PATH=/var/log/geoapi/app.log  # Path for file logging

# Observability settings
GEOAPI_ENABLE_TRACING=false  # Enable distributed tracing
GEOAPI_TRACING_PROVIDER=otlp  # otlp, jaeger, honeycomb
GEOAPI_TRACING_ENDPOINT=http://localhost:4317  # Endpoint for tracing exporter
```

### Log Formats

#### Development Mode (Console)
```
2025-06-29 10:15:23,456 [INFO] geoapi.request:42 - Request started: GET /api/v1/links
2025-06-29 10:15:23,512 [INFO] geoapi.request:98 - Request completed: GET /api/v1/links - 200
```

#### Production Mode (JSON)
```json
{
  "timestamp": "2025-06-29T10:15:23.456Z",
  "level": "INFO",
  "message": "Request completed: GET /api/v1/links - 200",
  "logger": "geoapi.request",
  "location": {
    "module": "logging_middleware",
    "function": "dispatch",
    "line": 98
  },
  "correlation_id": "a1b2c3d4-e5f6-7890-abcd-1234567890ab",
  "http": {
    "method": "GET",
    "url": "http://localhost:8000/api/v1/links",
    "status_code": 200,
    "response_time": 0.056,
    "request_id": "a1b2c3d4-e5f6-7890-abcd-1234567890ab"
  },
  "event": "request_completed",
  "performance": {
    "response_time": 0.056
  }
}
```

### Usage in Code

```python
# In FastAPI endpoints
@app.get("/items/{item_id}")
async def get_item(
    item_id: int, 
    logger: ContextLogger = Depends(get_request_logger)
):
    logger.info(f"Processing item {item_id}")
    
    # Add context for this specific operation
    operation_logger = logger.with_context({"operation": "get_item"})
    operation_logger.debug("Detailed operation info", extra={"item_id": item_id})
    
    return {"item_id": item_id}
```

---

## ğŸ”§ Development & Troubleshooting

### Common Development Tasks

```bash
# Start development environment
make start && make run-api-dev

# Run tests while developing
make test                    # Quick unit tests
make test-coverage          # Tests with coverage report
make test-api               # Test API endpoints

# Code quality checks
make format                 # Format code with Black
make type-check            # Type checking with mypy
make sort-imports          # Sort imports with isort
make quality-check         # All quality checks combined
```

### Troubleshooting Guide

#### API Not Responding
```bash
make api-status            # Check status
make logs                  # View logs
make restart-api           # Restart API process
```

#### Container Issues
```bash
make restart               # Restart all containers
make logs                  # Check container logs
make shell                 # Debug inside container
```

#### Database Issues
```bash
make db-shell              # Open PostgreSQL shell
make health-check          # Check database connectivity
make clean-db              # Clean database (careful!)
```

#### Development Issues
```bash
make clean-empty-files     # Remove VS Code empty files
make clean-pycache         # Clean Python cache
make format                # Fix code formatting
```

---

## ğŸš§ Future Work

[![Production Ready](https://img.shields.io/badge/Status-MVP%20Complete-success.svg?style=flat-square&logo=checkmarx&logoColor=white)](#future-work)
[![Roadmap](https://img.shields.io/badge/Roadmap-Production%20Features-blue.svg?style=flat-square&logo=roadmap&logoColor=white)](#production-roadmap)

This project represents a **comprehensive MVP** demonstrating advanced data engineering and API development capabilities. The following enhancements would be prioritized for production deployment:

### ğŸ”Œ **API Enhancement**
- **Pagination Optimization**: Cursor-based pagination for large result sets
- **Response Caching**: Redis-based intelligent caching with geospatial awareness
- **API Versioning**: Semantic versioning strategy with backward compatibility

### ğŸ—ï¸ **Infrastructure & DevOps**
- **Microservices Architecture**: Domain-driven service decomposition
- **Container Orchestration**: Kubernetes deployment with auto-scaling
- **CI/CD Pipeline**: Automated testing, security scanning, and deployment
- **Multi-Environment Setup**: Development, staging, and production environments

---

## ğŸ” Security Considerations

[![Security](https://img.shields.io/badge/Security-Enterprise%20Grade-red.svg?style=flat-square&logo=security&logoColor=white)](#security-implementation)
[![Compliance](https://img.shields.io/badge/Compliance-Ready-blue.svg?style=flat-square&logo=shield&logoColor=white)](#compliance-features)

> **âš ï¸ Note**: This MVP focuses on technical architecture and data engineering capabilities. Production security implementation would include comprehensive security measures detailed below.

### ğŸ›¡ï¸ **Authentication & Authorization**

- **Authentication**:
  - JWT/OAuth 2.0 with refresh tokens
  - Role-based access control (RBAC)
  - API key management with rate limiting

- **Authorization**:
  - Fine-grained permissions (read/write/admin)
  - Resource-level access control

### ğŸš¨ **API Security**
- **Input Validation**: Comprehensive sanitization and validation
- **SQL Injection Prevention**: Parameterized queries and ORM protection
- **Rate Limiting**: Adaptive rate limiting with DDoS protection
- **CORS Configuration**: Strict cross-origin resource sharing policies
- **Security Headers**: Implementation of security headers (HSTS, CSP, etc.)

---

## âš¡ Performance & Scalability

[![Performance](https://img.shields.io/badge/Performance-Enterprise%20Scale-green.svg?style=flat-square&logo=speedtest&logoColor=white)](#performance-optimization)
[![Scalability](https://img.shields.io/badge/Scalability-Cloud%20Native-blue.svg?style=flat-square&logo=kubernetes&logoColor=white)](#horizontal-scaling)

> **ğŸ“Š Current State**: The MVP efficiently handles 1.3M+ records with optimized chunk processing. Production scaling would implement the enterprise-grade solutions below.

### ğŸ—ï¸ **Horizontal Scaling Architecture**

- **Database Tier**:
  - PostgreSQL cluster with read replicas
  - Horizontal partitioning by geographic regions
  - Connection pooling with PgBouncer
  - Automated failover and backup strategies

- **Application Tier**:
  - Microservices with independent scaling
  - Load balancing
  - Auto-scaling based on CPU/memory metrics
  
- **Caching Layer**:
  - Redis cluster for session management
  - Geospatial query result caching
  - CDN for static assets and documentation
  - Application-level caching strategies

### ğŸ“Š **Database Optimization**
- **Query Optimization**: Advanced query planning and execution optimization
- **Partitioning**: Time-based and geographic data partitioning
- **Materialized Views**: Pre-computed aggregations for complex analytics

### ğŸ”§ **Application Performance**
- **Async Processing**: Non-blocking I/O with FastAPI async capabilities
- **Background Tasks**: Celery-based distributed task processing
- **Memory Management**: Advanced garbage collection and memory profiling
- **Code Optimization**: Performance profiling and algorithmic improvements

---
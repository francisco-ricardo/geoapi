# GeoSpatial Links API

A robust geospatial REST API built with **FastAPI**, **SQLAlchemy**, **PostgreSQL/PostGIS**, and **Pydantic** for traffic data analysis and visualization.

## üöÄ Quick Start (For Interviewers)

### Prerequisites
- Docker and Docker Compose installed on your host machine
- Clone this repository

### Setup and Run
```bash
# 1. Start the containers (database + API)
make start

# 2. Complete setup (tables + data ingestion)
make setup

# 3. Access the API
# - API Server: http://localhost:8000
# - API Documentation: http://localhost:8000/docs
# - Health Check: http://localhost:8000/health
```

### Alternative Commands
```bash
# Step by step setup
make start              # Start containers
make create-tables      # Create database tables
make ingest-data        # Load Parquet datasets

# Development commands
make logs              # View container logs
make shell             # Open shell in API container
make db-shell          # Open PostgreSQL shell
make test              # Run tests
make health-check      # Check system health

# Data validation
make validate-ingestion # Validate data ingestion integrity
```

### Access Points
- **API Server**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs  
- **Health Check**: http://localhost:8000/health

## ÔøΩ Technologies

- **Backend**: FastAPI, SQLAlchemy 2.0, Pydantic v2
- **Database**: PostgreSQL + PostGIS (with automatic table creation)
- **Geospatial**: GeoAlchemy2, PostGIS, GeoJSON
- **Testing**: pytest, TDD approach
- **DevOps**: Docker, DevContainer, automated setup

## ÔøΩ Project Structure

```
app/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration with Pydantic Settings
‚îÇ   ‚îî‚îÄ‚îÄ database.py        # Engine/session factory (SQLite/PostgreSQL)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ link.py           # Road links model (with PostGIS geometry)
‚îÇ   ‚îî‚îÄ‚îÄ speed_record.py   # Speed measurements model
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îú‚îÄ‚îÄ link.py           # Pydantic schemas for links
‚îÇ   ‚îî‚îÄ‚îÄ speed_record.py   # Pydantic schemas for speed records
‚îú‚îÄ‚îÄ api/v1/
‚îÇ   ‚îî‚îÄ‚îÄ links.py          # API endpoints implementation
‚îî‚îÄ‚îÄ services/             # Business logic layer

scripts/
‚îú‚îÄ‚îÄ setup/
‚îÇ   ‚îî‚îÄ‚îÄ complete_setup.py # Automated project setup
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îî‚îÄ‚îÄ create_tables.py  # Database initialization
‚îú‚îÄ‚îÄ demo/
‚îÇ   ‚îú‚îÄ‚îÄ schemas_basic.py  # Basic schema demonstration
‚îÇ   ‚îî‚îÄ‚îÄ schemas_complete.py # Complete schema guide
‚îî‚îÄ‚îÄ testing/
    ‚îú‚îÄ‚îÄ run_tests.py      # Test runner
    ‚îú‚îÄ‚îÄ run_tests_by_category.py # Category-based tests
    ‚îî‚îÄ‚îÄ test_endpoints.py # API endpoint testing

tests/
‚îú‚îÄ‚îÄ conftest.py           # Shared test fixtures
‚îú‚îÄ‚îÄ test_*.py            # Unit tests
‚îî‚îÄ‚îÄ test_models/         # Model-specific tests
```

## üß™ Testing System

The project features a **comprehensive testing system** with multiple categories:

### Run Tests
```bash
# All tests
make test

# By category
python scripts/testing/run_tests_by_category.py basic    # No database required
python scripts/testing/run_tests_by_category.py schema   # Pydantic validation
python scripts/testing/run_tests_by_category.py database # PostgreSQL required
python scripts/testing/run_tests_by_category.py all      # Complete suite
```

### API Testing
```bash
# Test endpoints manually
python scripts/testing/test_endpoints.py
```

# Mostra ajuda detalhada sobre os testes
python run_tests.py --help-tests
```

### Executar Todos os Testes (Produ√ß√£o)

```bash
# Executa TODOS os testes (requer PostgreSQL/PostGIS)
python run_tests.py --all
```

### üìä Cobertura de Testes

- **24 testes** compat√≠veis com SQLite (desenvolvimento local)
- **Cobertura completa** da l√≥gica de neg√≥cio dos modelos
- **Testes de configura√ß√£o** com vari√°veis de ambiente
- **Testes de database factory** com cache e health checks
- **Testes isolados** para PostGIS (apenas em ambiente PostgreSQL)

### üéØ Estrat√©gia de Testes

1. **Modelos Simplificados**: Vers√µes dos modelos sem geometria para testes com SQLite
2. **Testes de Estrutura**: Validam campos, relacionamentos e m√©todos sem banco
3. **Testes de Integra√ß√£o**: Executados apenas em ambiente PostgreSQL/PostGIS
4. **TDD Friendly**: Todos os testes essenciais funcionam localmente

## üîç Valida√ß√£o de Dados

### Valida√ß√£o da Integridade da Ingest√£o

O projeto inclui um sistema robusto de valida√ß√£o de dados para garantir a integridade ap√≥s a ingest√£o:

```bash
# Validar integridade dos dados ap√≥s ingest√£o
make validate-ingestion
```

#### O que √© validado:

**1. Dados de Links:**
- Geometrias v√°lidas (GeoJSON/WKT)
- Coordenadas consistentes (SRID 4326) 
- Campos obrigat√≥rios preenchidos
- Unicidade dos link_ids

**2. Dados de Velocidade:**
- Refer√™ncias v√°lidas para links existentes
- Timestamps em formato correto (UTC)
- Valores de velocidade dentro de limites razo√°veis
- Per√≠odos de tempo categorizados corretamente

**3. Integridade Referencial:**
- Todos os speed_records referenciam links v√°lidos
- Geometrias PostGIS v√°lidas e consistentes
- SRID uniforme em todas as geometrias

**4. Consist√™ncia Estat√≠stica:**
- Contagens de registros esperadas
- M√©dias de velocidade por per√≠odo coerentes
- Distribui√ß√£o temporal adequada

#### Exemplo de Output:
```
[PASS] Link geometries validation passed
[PASS] Speed records validation passed  
[PASS] All speed records have valid link references
[PASS] All geometries use consistent SRID: 4326
[PASS] Average speed for AM Peak matches: 35.94 mph
[PASS] ALL VALIDATIONS PASSED - Data ingestion is accurate

*** Data integrity confirmed! The ingestion process worked correctly. ***
```

### Outros Comandos de Valida√ß√£o

```bash
make analyze-data      # Analisar datasets Parquet originais
make verify-db         # Verificar estado do banco de dados
make verify-postgis    # Verificar dados espaciais PostGIS
```

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente

Copie `.env.example` para `.env` e configure:

```bash
# Database
DATABASE_URL="postgresql://user:pass@localhost:5432/geoapi"
DATABASE_URL_TEST="sqlite:///./test.db"

# API
API_TITLE="GeoAPI"
API_VERSION="1.0.0"
DEBUG=true

# Dados
MAPBOX_TOKEN="pk.your_token_here"  # Opcional
```

### DevContainer

O projeto est√° configurado para usar **DevContainer** com todas as depend√™ncias:

```bash
# Inicia o ambiente de desenvolvimento
docker-compose -f docker-compose-dev.yml up -d
```

## üîß Desenvolvimento

### 1. Configurar Ambiente

```bash
# Instalar depend√™ncias
pip install -r requirements-dev.txt

# Configurar Python environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

### 2. Executar Testes

```bash
# Desenvolvimento local (SQLite)
python run_tests.py --sqlite

# Ambiente completo (PostgreSQL)
python run_tests.py --all
```

### 3. Estrutura dos Modelos

#### Link (Links Vi√°rios)
```python
class Link(Base):
    id: int (PK)
    link_id: str (√∫nico)
    road_name: str (opcional)
    geometry: Geometry (Point/LineString, nullable para testes)
    speed_records: relationship -> SpeedRecord[]
```

#### SpeedRecord (Registros de Velocidade)
```python
class SpeedRecord(Base):
    id: int (PK)
    link_id: int (FK -> Link.id)
    timestamp: datetime
    speed_kph: float
    period: str (morning/afternoon/evening/night)
    link: relationship -> Link
```

## üìà Pr√≥ximos Passos

- [ ] Implementar endpoints FastAPI (`/api/v1/`)
- [ ] Criar schemas Pydantic para serializa√ß√£o
- [ ] Implementar servi√ßos de ingest√£o de dados Parquet
- [ ] Desenvolver notebook Jupyter para an√°lise com Mapbox
- [ ] Adicionar testes de integra√ß√£o da API
- [ ] Configurar CI/CD com GitHub Actions

## üèóÔ∏è Arquitetura

O projeto segue os princ√≠pios de **Clean Architecture**, **SOLID**, e **KISS**:

- **Factory Pattern** para database connections
- **Dependency Injection** para configura√ß√£o
- **Repository Pattern** (a implementar)
- **Clean Code** com tipagem forte
- **TDD** com cobertura abrangente

## üìö Documenta√ß√£o

- **FastAPI**: Documenta√ß√£o autom√°tica em `/docs` (Swagger)
- **Modelos**: Documentados com docstrings e type hints
- **Testes**: Casos de teste descritivos e bem organizados

---

**Status Atual**: ‚úÖ Base s√≥lida implementada, pronto para desenvolvimento da API

## üìö Lessons Learned

### üöÄ Performance Optimization: Chunk Processing

Durante o desenvolvimento, implementamos **chunk processing** para otimizar a ingest√£o de dados:

#### **Problema Inicial**
- Datasets grandes (1.2M+ registros) causavam problemas de mem√≥ria
- Processamento sequencial era lento para grandes volumes

#### **Solu√ß√£o Implementada**
```python
# Chunk processing otimizado
def process_links_chunked(df, chunk_size=1000):
    """Process links data in chunks to avoid memory issues"""
    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]
    
    for i, chunk in enumerate(chunks):
        # Process each chunk separately
        chunk_data = prepare_link_data(chunk)
        insert_batch(session, Link, chunk_data)
        
def process_speed_records_chunked(df, chunk_size=5000):
    """Manual chunking for speed records processing"""
    total_records = len(df)
    
    for start_idx in range(0, total_records, chunk_size):
        end_idx = min(start_idx + chunk_size, total_records)
        chunk = df.iloc[start_idx:end_idx]
        
        # Process chunk with optimized batch insert
        process_chunk_data(chunk)
```

#### **Resultados Obtidos**
- ‚úÖ **Uso de Mem√≥ria**: Reduzido significativamente (chunks de 5K registros)
- ‚úÖ **Performance**: Ingest√£o de 1.2M registros em ~7 minutos
- ‚úÖ **Confiabilidade**: Zero falhas de mem√≥ria ou timeouts
- ‚úÖ **Monitoramento**: Progress tracking em tempo real

#### **M√©tricas de Performance**
```
Links: 100,924 registros em chunks de 1,000
Speed Records: 1,239,946 registros em chunks de 5,000
Tempo total: ~7 minutos
Taxa: ~3,000 registros/segundo
```

#### **Li√ß√µes Aprendidas**
1. **Chunk Size Matters**: 5K registros = sweet spot entre mem√≥ria e performance
2. **Batch Inserts**: SQLAlchemy bulk operations s√£o 10x mais r√°pidas
3. **Memory Management**: Chunking evita OutOfMemory em datasets grandes
4. **Progress Tracking**: Feedback visual melhora UX durante ingest√£o
5. **Error Handling**: Chunks permitem retry granular em caso de falhas

### üõ†Ô∏è Technical Implementation

```python
# Otimiza√ß√£o principal no script de ingest√£o
CHUNK_SIZE = 5000  # Testado e otimizado

# Loop principal otimizado
for start_idx in range(0, total_records, CHUNK_SIZE):
    chunk = df.iloc[start_idx:start_idx + CHUNK_SIZE]
    
    # Bulk insert com SQLAlchemy
    session.bulk_insert_mappings(SpeedRecord, chunk_data)
    session.commit()
    
    # Progress tracking
    print(f"Chunk {chunk_num}: {len(chunk_data)} records processed")
```

---
